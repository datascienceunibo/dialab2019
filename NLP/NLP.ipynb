{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Laboratorio: Natural Language Processing\n",
    "\n",
    "**Programmazione di Applicazioni Data Intensive**  \n",
    "Laurea in Ingegneria e Scienze Informatiche  \n",
    "DISI - Università di Bologna, Cesena\n",
    "\n",
    "Proff. Gianluca Moro, Roberto Pasolini  \n",
    "`nome.cognome@unibo.it`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "- Con _Natural Language Processing_ (NLP) ci si riferisce all'insieme di tecniche per il processamento di testo in linguaggio naturale (inglese, italiano, ...)\n",
    "- Obiettivo del NLP è estrarre informazioni di alto livello dal testo o convertirlo in una forma strutturata (es. vettori e matrici) trattabile da altri algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK\n",
    "\n",
    "- _NLTK_ (_Natural Language Toolkit_) è una delle principali librerie Python per il trattamento di testi in linguaggio naturale\n",
    "- Fornisce diversi algoritmi, spesso usati come componenti per pre-processare documenti di testo nell'ambito di processi di analisi dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alcuni algoritmi richiedono dati o modelli di conoscenza non distribuiti di default con la libreria (per limitarne le dimensioni)\n",
    "- Dove necessari, viene usata la funzione `download` per scaricare i dati se non già presenti su disco\n",
    "  - i dati scaricati sono salvati in una directory `nltk_data` nella propria home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentazione\n",
    "\n",
    "- La _segmentazione_ (_tokenization_) consiste nella scomposizione di un testo in una **sequenza di elementi** (_token_)\n",
    "- Comunemente la segmentazione è usata per estrarre le **singole parole** da un testo, includendo opzionalmente numeri, segni di punteggiatura, ...\n",
    "- NLTK offre la funzione `word_tokenize` per scomporre una stringa in una lista di parole e segni di punteggiatura\n",
    "- La funzione utilizza un modello della lingua inglese per scomporre correttamente alcune parole\n",
    "- Usiamo la funzione `download` per scaricare tale modello (se non già scaricato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pasolini/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sia data una frase d'esempio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This isn't an example, or is it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...potremmo utilizzare il metodo `split` di Python per suddividere la frase in parole separate dagli spazi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', \"isn't\", 'an', 'example,', 'or', 'is', 'it?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando però `word_tokenize`, grazie alla conoscenza della lingua, sono correttamente separati segni di punteggiatura e anche parole composte come \"isn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"n't\", 'an', 'example', ',', 'or', 'is', 'it', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words e Vector Space Model\n",
    "\n",
    "- Nel modello _Bag of Words_ (BoW), un testo è rappresentato dall'**insieme delle parole in esso**\n",
    "  - non si considera il loro ordine nella frase\n",
    "- Definito un dizionario $D$ di parole distinte, possiamo rappresentare un testo (_documento_) con un vettore che associ ad ogni parola in $D$ il numero di occorrenze in esso\n",
    "- Il _Vector Space Model_ prevede di rappresentare un insieme di documenti in uno **spazio vettoriale** comune, dove **le dimensioni corrispondono ai termini** di un dizionario comune\n",
    "- Un insieme di documenti in uno spazio vettoriale è rappresentabile con una **_matrice documenti-termini_**, di cui ogni riga costituisce il vettore ricavato dal documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definire uno Spazio Vettoriale\n",
    "\n",
    "- Dato un insieme di documenti di testo (in questo caso semplici frasi)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"the sky is blue\",\n",
    "    \"sky is blue and sky is beautiful\",\n",
    "    \"the beautiful sky is so blue\",\n",
    "    \"i love blue cheese\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possiamo usare il filtro `CountVectorizer` fornito da scikit-learn per rappresentarli in uno spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Col metodo `fit_transform` costruiamo lo spazio vettoriale sulla base dei termini presenti nei documenti e otteniamo la matrice documenti-termini che li rappresenta\n",
    "  - scikit-learn include un algoritmo basilare per segmentare le parole, usato di default da `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = vect.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'oggetto `dtm` ottenuto è una _matrice sparsa_, una struttura dati che rappresenta una matrice memorizzando in modo esplicito solamente i valori diversi da 0\n",
    "  - in applicazioni reali tipiche, una matrice documenti-termini contiene molto meno del 10% di valori diversi da 0, si ottiene così un grande risparmio di memoria\n",
    "- **Attenzione:** la matrice sparsa è simile ad un `ndarray` ma con alcune differenze, ad es. l'operatore `*`  esegue il prodotto canonico tra matrici piuttosto che quello elemento per elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per visualizzare i valori della matrice, possiamo convertirla in un array NumPy col metodo `toarray`\n",
    "  - su questa matrice piccola non ci sono problemi di memoria, ma va usato con cautela su matrici più grandi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 2, 0, 2, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per ottenere l'elenco dei termini nel dizionario costruito, usare il metodo `get_feature_names`\n",
    "  - di default, ci sono tutte le parole distinte presenti nei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le parole sono ordinate in modo concorde con le colonne della matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the sky is blue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky is blue and sky is beautiful</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the beautiful sky is so blue</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love blue cheese</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  and  beautiful  blue  cheese  is  love  sky  \\\n",
       "the sky is blue                     0          0     1       0   1     0    1   \n",
       "sky is blue and sky is beautiful    1          1     1       0   2     0    2   \n",
       "the beautiful sky is so blue        0          1     1       0   1     0    1   \n",
       "i love blue cheese                  0          0     1       1   0     1    0   \n",
       "\n",
       "                                  so  the  \n",
       "the sky is blue                    0    1  \n",
       "sky is blue and sky is beautiful   0    0  \n",
       "the beautiful sky is so blue       1    1  \n",
       "i love blue cheese                 0    0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), index=docs, columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Col metodo `transform`, possiamo rappresentare ulteriori documenti nel medesimo spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"loving this blue sky today\"\n",
    "vect.transform([new_doc]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loving this blue sky today</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            and  beautiful  blue  cheese  is  love  sky  so  \\\n",
       "loving this blue sky today    0          0     1       0   0     0    1   0   \n",
       "\n",
       "                            the  \n",
       "loving this blue sky today    0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vect.transform([new_doc]).toarray(), index=[new_doc], columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si noti che alcune parole del nuovo documento (es. \"loving\") si perdono nella trasformazione, in quanto non previste nello spazio vettoriale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caso d'Uso: Classificazione di Recensioni\n",
    "\n",
    "- Sul Web sono continuamente pubblicate opinioni degli utenti, ad es. di film\n",
    "  - alcune di queste (es. su Amazon) sono etichettate con un numero di stelle, che indicano se sia positiva o negativa\n",
    "  - altre (es. messaggi sui forum) devono essere lette per capirne l'orientamento\n",
    "- Vogliamo addestrare un classificatore su recensioni etichettate come positive o negative, in modo che sia in grado di stimare l'orientamento di opinioni non etichettate\n",
    "- Utilizziamo un file di 10000 recensioni di film tratte da Amazon, a ciascuna delle quali è associato un punteggio da 1 a 5 stelle\n",
    "- Utilizzando il vector space model, possiamo addestrare un modello sui conteggi di tutte le parole presenti nei documenti\n",
    "- Carichiamo il file con le recensioni come visto in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"reviews.csv.gz\"):\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(\"https://git.io/vpaDt\", \"reviews.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Carichiamo il file come DataFrame con `read_csv`\n",
    "  - il file è compresso col formato GZIP, riconosciuto automaticamente da pandas dall'estensione \".gz\" (se non riconosciuto, specificare `compression=\"gzip\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"reviews.csv.gz\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il file contiene due colonne\n",
    "  - in `text` troviamo il testo della recensione\n",
    "  - in `stars` troviamo il numero di stelle date dall'utente, da 1 a 5\n",
    "- Visioniamo alcune righe del dataset, aumentando prima il numero di caratteri visualizzati per parola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>You seen one heist film, you seen them all. But every once in a while, somebody who really gives...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "9993  Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...   \n",
       "9994  Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...   \n",
       "9995  You seen one heist film, you seen them all. But every once in a while, somebody who really gives...   \n",
       "9996  Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...   \n",
       "9997  This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...   \n",
       "9998  I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...   \n",
       "9999  When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...   \n",
       "\n",
       "      stars  \n",
       "9993      4  \n",
       "9994      4  \n",
       "9995      5  \n",
       "9996      1  \n",
       "9997      3  \n",
       "9998      3  \n",
       "9999      5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analisi Esplorativa\n",
    "\n",
    "- Quante recensioni abbiamo per ciascun numero di stelle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4708\n",
       "4    2620\n",
       "3    1434\n",
       "2     704\n",
       "1     534\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"stars\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f22410abe48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMcAAAC4CAYAAABXcfjSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmHAYRgImHIaRgAmHYSRgwmEYCfw/IvH1EBA6mr8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22405ce358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews[\"stars\"].value_counts().sort_index().plot.pie(figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Quale lunghezza (in numero di caratteri) hanno le recensioni?\n",
    "  - il metodo `str.len` trasforma una serie di testi nelle rispettive lunghezze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f22405ce518>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqRuhockqZvhIUnqZnhIkroZHpKkbv8f5pa5+EiFusoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2240b135c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews[\"text\"].str.len().plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recensioni Positive e Negative\n",
    "\n",
    "- Per semplificare l'analisi, riduciamo le 5 possibili etichette a due\n",
    "- Aggiungiamo una colonna \"label\" dove etichettiamo\n",
    "  - come _positive_ (\"pos\") le recensioni con 4 o 5 stelle\n",
    "  - come _negative_ (\"neg\") quelle con 3 stelle o meno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reviews[\"label\"] = np.where(reviews[\"stars\"] >= 4, \"pos\", \"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il numero di recensioni che si ottengono di ciascun tipo è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    7328\n",
       "neg    2672\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>You seen one heist film, you seen them all. But every once in a while, somebody who really gives...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "9993  Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...   \n",
       "9994  Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...   \n",
       "9995  You seen one heist film, you seen them all. But every once in a while, somebody who really gives...   \n",
       "9996  Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...   \n",
       "9997  This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...   \n",
       "9998  I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...   \n",
       "9999  When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...   \n",
       "\n",
       "      stars label  \n",
       "9993      4   pos  \n",
       "9994      4   pos  \n",
       "9995      5   pos  \n",
       "9996      1   neg  \n",
       "9997      3   neg  \n",
       "9998      3   neg  \n",
       "9999      5   pos  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classificazione di Testi\n",
    "\n",
    "- La matrice documenti-termini ha la forma di un dataset\n",
    "  - ogni riga rappresenta un esempio (un testo) da classificare\n",
    "  - ogni colonna rappresenta una variabile che caratterizza gli esempi\n",
    "- Possiamo quindi addestrare un modello di classificazione su tale matrice per stimare l'orientamento delle recensioni\n",
    "- Iniziamo suddividendo come al solito i dati in training set (70\\%) e validation set (30\\%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "reviews_train, reviews_val = \\\n",
    "    train_test_split(reviews, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Definiamo quindi lo spazio vettoriale in cui rappresentare le recensioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Costruiamo lo spazio vettoriale sul training set (quindi con le parole contenute in esso) e otteniamone la matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_train = vect.fit_transform(reviews_train[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otteniamo uno spazio con tante dimensioni quante le parole distinte nelle recensioni di training, ovvero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51772"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Esempi di parole estratte sono..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abides',\n",
       " 'abiding',\n",
       " 'abigail',\n",
       " 'abilene',\n",
       " 'abilites',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'abition',\n",
       " 'abject']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La matrice è estremamente sparsa: la percentuale di termini non 0 è molto bassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002740529905850931"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.astype(bool).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rappresentiamo i documenti del validation set nello stesso spazio vettoriale, ottenendone la corrispondente matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_val = vect.transform(reviews_val[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A questo punto creiamo un modello di classificazione e addestriamolo passando la matrice documenti-termini e le etichette delle recensioni relative al training set\n",
    "  - usiamo ad es. la regressione logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrm = LogisticRegression(C=10)\n",
    "lrm.fit(dtm_train, reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando matrice ed etichette del validation set possiamo valutare l'accuratezza del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79066666666666663"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.score(dtm_val, reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Una volta addestrato il modello, possiamo usarlo per stimare come \"positive\" o \"negative\" altre recensioni\n",
    "- Prendiamo ad esempio le seguenti due frasi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_reviews = [\"What an awesome movie!\", \"It was really boring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una volta estratta la loro rappresentazione nello spazio vettoriale costruito sui documenti di training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_new = vect.transform(new_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...usando il metodo `predict` per ottenere le classi predette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict(dtm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Oltre ad una risposta secca (\"positiva\" o \"negativa\"), i classificatori possono fornire in genere una **distribuzione di probabilità** tra le possibili classi\n",
    "- Per ottenerla usiamo il metodo `predict_proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02294871,  0.97705129],\n",
       "       [ 0.59911844,  0.40088156]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict_proba(dtm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ogni elemento in riga i e colonna j della matrice indica la probabilità che la classe del i-esimo elemento da classificare sia la j-esima\n",
    "- L'ordine in cui sono considerate le classi si può ottenere dall'attributo `classes_` del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Vediamo la matrice con righe e colonne etichettate per maggiore chiarezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>What an awesome movie!</th>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.977051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It was really boring</th>\n",
       "      <td>0.599118</td>\n",
       "      <td>0.400882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             neg       pos\n",
       "What an awesome movie!  0.022949  0.977051\n",
       "It was really boring    0.599118  0.400882"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lrm.predict_proba(dtm_new),\n",
    "             index=new_reviews,\n",
    "             columns=lrm.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vediamo quindi che il modello è sicuro al 98\\% circa che la prima frase sia positiva e al 60\\% che la seconda sia negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametri del modello\n",
    "\n",
    "- Possiamo accedere ai coefficienti lineari assegnati al modello per ciascuna variabile, ovvero per ciascun termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.16224281e-01,  -1.29820886e-01,   4.16113901e-02,\n",
       "        -1.02112749e-05])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.coef_[0, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possiamo creare una serie che associ a ciascuno di questi coefficienti il termine corrispondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = pd.Series(lrm.coef_[0], index=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ordinando questa serie in base ai valori, vediamo quali i coefficienti più alti e più bassi e da questi quali parole contribuiscano di più a rendere una recensione positiva o negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- I coefficienti più bassi fanno tendere la decisione alla classe \"0\", in questo caso le recensioni negative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "waste          -2.871880\n",
       "wondering      -2.609906\n",
       "disappointed   -2.484919\n",
       "horrible       -2.333217\n",
       "terrible       -2.329850\n",
       "3rd            -2.111883\n",
       "miscast        -2.055064\n",
       "worst          -2.015429\n",
       "decent         -1.992278\n",
       "hype           -1.898268\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ...mentre quelli più alti fanno tendere alla classe \"1\", le positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "realistic    1.822469\n",
       "awesome      1.842950\n",
       "gem          1.848051\n",
       "excellent    1.884631\n",
       "powerful     1.911671\n",
       "amazing      1.921509\n",
       "perfect      1.986372\n",
       "highly       2.006586\n",
       "hilarious    2.009481\n",
       "tad          2.500362\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline per Pre-processamento Testo e Classificazione\n",
    "\n",
    "- Come visto in precedenza, una _pipeline_ di scikit-learn permette di concatenare ad un modello di predizione uno o più filtri che sono applicati automaticamente a tutti i dati\n",
    "- Costruendo una pipeline con il `CountVectorizer` e il modello `LogisticRegression`, otteniamo un classificatore che accetta in input direttamente i testi, calcolando in automatico la loro conversione nello spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Addestrando e testando questo modello composito sugli stessi training e validation set, otteniamo gli stessi risultati senza estrarre manualmente le matrici documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79066666666666663"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(new_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## tf.idf\n",
    "\n",
    "- Abbiamo visto che il `CountVectorizer` estrae una matrice in cui ogni elemento è il numero di occorrenze di un termine in un documento\n",
    "- Esistono però metodi più avanzati per pesare la rilevanza di un termine in un documento\n",
    "- È di uso comune il _tf.idf_ (_term frequency-inverse document frequency_), pari al prodotto di due fattori (di cui esistono diverse formulazioni)\n",
    "  - il _tf_ indica l'**importanza locale** di un termine in un documento ed è pari al numero di occorrenze (o al suo logaritmo)\n",
    "  - l'_idf_ indica l'**importanza globale** di un termine, tanto più alta quanto più il termine è poco comune nell'insieme complessivo dei documenti\n",
    "  - comunemente, una volta calcolati tutti i pesi, **ciascun vettore è normalizzato** in modo da avere norma euclidea pari a 1, per appianare differenze di pesi tra documenti più o meno lunghi\n",
    "- `TfidfVectorizer` è un filtro utilizzabile come alternativa a `CountVectorizer` per estrarre matrici documenti-termini basate sul tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Calcoliamo ad esempio la matrice dei tf.idf per l'insieme di 4 frasi usate sopra..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the sky is blue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.603137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky is blue and sky is beautiful</th>\n",
       "      <td>0.440516</td>\n",
       "      <td>0.347308</td>\n",
       "      <td>0.229880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the beautiful sky is so blue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432026</td>\n",
       "      <td>0.285953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349762</td>\n",
       "      <td>0.54797</td>\n",
       "      <td>0.432026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love blue cheese</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  beautiful      blue    cheese  \\\n",
       "the sky is blue                   0.000000   0.000000  0.399210  0.000000   \n",
       "sky is blue and sky is beautiful  0.440516   0.347308  0.229880  0.000000   \n",
       "the beautiful sky is so blue      0.000000   0.432026  0.285953  0.000000   \n",
       "i love blue cheese                0.000000   0.000000  0.346182  0.663385   \n",
       "\n",
       "                                        is      love       sky       so  \\\n",
       "the sky is blue                   0.488291  0.000000  0.488291  0.00000   \n",
       "sky is blue and sky is beautiful  0.562351  0.000000  0.562351  0.00000   \n",
       "the beautiful sky is so blue      0.349762  0.000000  0.349762  0.54797   \n",
       "i love blue cheese                0.000000  0.663385  0.000000  0.00000   \n",
       "\n",
       "                                       the  \n",
       "the sky is blue                   0.603137  \n",
       "sky is blue and sky is beautiful  0.000000  \n",
       "the beautiful sky is so blue      0.432026  \n",
       "i love blue cheese                0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "dtm = vect.fit_transform(docs)\n",
    "pd.DataFrame(dtm.toarray(), index=docs, columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ad es. nell'ultimo documento \"cheese\" ha un peso maggiore di \"blue\" in quanto è una parola meno comune e quindi più discriminante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per testare l'uso del tf.idf nel classificatore di recensioni, sostituiamo il `CountVectorizer` con il `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Addestrando il modello sul training set e valutando l'accuratezza sul validation set come sopra..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82499999999999996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...otteniamo un miglioramento dei risultati di qualche punto percentuale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riduzione della Dimensionalità\n",
    "\n",
    "- Il numero di dimensioni generate considerando tutti i termini distinti presenti in tutti i documenti, come anche visto sopra, è molto alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51772"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All'aumentare del numero di documenti, una tale quantità di dimensioni può comportare tempi di calcolo e uso di memoria eccessivi\n",
    "- Esistono però modi per ridurre il numero di dimensioni con effetti spesso trascurabili sull'accuratezza finale del modello\n",
    "- Ad esempio, impostando il parametro `min_df` di `TfidfVectorizer` (o `CountVectorizer`), limitiamo le parole nel dizionario dello spazio vettoriale a quelle presenti in almeno _N_ documenti di training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creiamo ad esempio un modello limitato alle parole che appaiono in almeno 3 documenti di training distinti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3)),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il numero di feature è molto inferiore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21063"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...ma l'accuratezza è quasi identica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82199999999999995"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rimozione Stopword\n",
    "\n",
    "- Le _stopword_ sono le parole usate nei testi per rendere complete le frasi, ma che prese da sole non danno alcuna informazione sulla semantica del testo\n",
    "  - sono stopword ad es. articoli (\"le\"), preposizioni (\"per\"), congiunzioni (\"ma\"), ...\n",
    "- Nella rappresentazione BOW di documenti è comune rimuovere a priori le stopword, in quanto non informative del contenuto\n",
    "- Esistono diverse liste di stopword, NLTK ne integra alcune per diverse lingue\n",
    "- Reperiamo la lista delle stopword inglesi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pasolini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La lista include ad esempio le parole..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Nei vectorizer, possiamo configurare una lista di parole da escludere con un parametro `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, stop_words=stoplist)),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rimuovendo le stopword, così come accade impostando `min_df`, otteniamo generalmente una riduzione delle feature con variazioni trascurabili di accuratezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20922"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80966666666666665"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## n-gram\n",
    "\n",
    "- Un _n-gram_ è una **sequenza di _n_ parole consecutive** presenti in un testo\n",
    "  - nei casi più comuni con n pari a 2 o 3 si parla rispettivamente di _bigram_ o _trigram_\n",
    "  - ad es., nella frase \"vado a New York\", i bigram sono \"vado a\", \"a New\" e \"New York\"\n",
    "- Così come le parole singole, anche gli n-gram possono essere usati come **feature per rappresentare i documenti**\n",
    "  - alcuni n-gram possono essere significativi, rappresentando un termine composto da più parole (es. \"New York\")\n",
    "  - ne rimangono però molti senza un significato specifico (es. \"a New\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Impostando in un vectorizer l'attributo `ngram_range` ad una **tupla `(a, b)`**, sono usate come feature le possibili **sequenze contenenti dalle a alle b parole**\n",
    "  - l'impostazione di default è `(1, 1)`, per cui sono selezionate solo parole singole\n",
    "  - impostando invece ad es. `(1, 2)` si selezionano sia le parole singole che i bigram\n",
    "  - introducendo gli n-gram **il numero di termini distinti aumenta di molto**: è importante ridurli specificando un `min_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, ngram_range=(1, 2))),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100711"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82333333333333336"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- I termini estratti includono parole singole e bigram significativi e non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zone and',\n",
       " 'zone feel',\n",
       " 'zone the',\n",
       " 'zones',\n",
       " 'zoo',\n",
       " 'zooey',\n",
       " 'zooey deschanel',\n",
       " 'zoolander',\n",
       " 'zoom',\n",
       " 'zooming']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_steps[\"vectorizer\"].get_feature_names()[-15:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## POS Tagging\n",
    "\n",
    "- Ad ogni parola in una frase è associata una **classe grammaticale**, detta _Part of Speech_ (POS)\n",
    "  - ad alto livello le POS sono **_nome_, _verbo_, _aggettivo_, ecc.**\n",
    "  - spesso si effettuano ulteriori distinzioni, ad es. nomi singolari e plurali\n",
    "  - le POS possono includere anche i segni di punteggiatura\n",
    "- Il _POS tagging_ etichetta ogni token di una sequenza (ottenuta dalla segmentazione) con il suo POS\n",
    "  - è un processo non banale, in quanto una stessa parola può avere diverse POS a seconda del contesto (es. \"letto\" può essere nome o verbo)\n",
    "- La funzione `pos_tag` prende una sequenza di token e restituisce una lista di tuple `(token, tag)`\n",
    "  - i possibili tag sono elencati quì: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pasolini/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(\"This isn't an example, or is it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Il POS tagging si può usare ad esempio per differenziare i termini in un BOW a seconda dell'uso che se ne fa nella frase\n",
    "  - ad esempio, invece di avere un'unica feature \"set\", si possono avere feature \"set (nome)\" e \"set (verbo)\"\n",
    "- Creiamo ad esempio una funzione che usa NLTK per segmentare un documento di testo e trasforma ciascun termine in una stringa \"`termine/POS`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_pos(text):\n",
    "    return [\"{}/{}\".format(token.lower(), tag) for token, tag\n",
    "            in nltk.pos_tag(nltk.tokenize.word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this/DT',\n",
       " 'is/VBZ',\n",
       " \"n't/RB\",\n",
       " 'an/DT',\n",
       " 'example/NN',\n",
       " ',/,',\n",
       " 'or/CC',\n",
       " 'is/VBZ',\n",
       " 'it/PRP',\n",
       " '?/.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_pos(\"This isn't an example, or is it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In un vectorizer, possiamo specificare come parametro `tokenizer` una funzione per estrarre parole dai documenti da usare in alternativa a quella predefinita di scikit-learn\n",
    "- Usiamo la funzione che abbiamo definito sopra, in modo da usare gli elementi \"`termine/POS`\" come feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, tokenizer=tokenize_with_pos)),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26793"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81466666666666665"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatizzazione\n",
    "\n",
    "- Estraendo dai documenti tutte le parole nella forma in cui sono scritte, otteniamo spesso molteplici forme di uno stesso _lemma_ (parola di un vocabolario)\n",
    "  - es. le parole \"estraggo\", \"estraendo\", \"estratto\", ... sono tutte coniugazioni diverse del verbo \"estrarre\"\n",
    "- La _lemmatizzazione_ è il processo che converte ciascuna parola di un testo nella sua forma base, quella in cui sono presentati in un dizionario\n",
    "  - nomi al singolare, verbi all'infinito, ...\n",
    "- In questo modo **raggruppiamo gruppi di termini simili**, riducendo la dimensionalità dello spazio senza perdita di informazione rilevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In NLTK possiamo creare un oggetto `WordNetLemmatizer` per lemmatizzare singole parole, delle quali va specificato il POS\n",
    "  - i POS da passare sono n=nome, v=verbo, a=aggettivo, r=avverbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pasolini/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mouse', 'go')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "wnl.lemmatize(\"mice\", \"n\"), wnl.lemmatize(\"went\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creiamo una funzione che segmenti le parole di un testo eseguendo la lemmatizzazione dove possibile\n",
    "  - creiamo un dizionario con le corrispondenze tra le prime lettere dei POS in Penn Treebank visti sopra e questi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penn_to_wn = {\"N\": \"n\", \"V\": \"v\", \"J\": \"a\", \"R\": \"r\"}\n",
    "def tokenize_with_lemmatization(text):\n",
    "    return [(wnl.lemmatize(token, penn_to_wn[tag[0]]) if tag[0] in penn_to_wn else token)\n",
    "            for token, tag in nltk.pos_tag(nltk.tokenize.word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'have', 'show', 'many', 'example', '!']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_lemmatization(\"We have shown many examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La funzione ha estratto le parole, ma portando il verbo \"show\" e il nome \"example\" alle forme base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Usiamo la funzione come `tokenizer` per la creazione dello spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, tokenizer=tokenize_with_lemmatization)),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anche in questo modo otteniamo un numero di feature inferiore, tuttavia il tempo impiegato per il processamento del testo è elevato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18177"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82033333333333336"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "\n",
    "- Un algoritmo di _stemming_ estrae da una parola la sua **radice morfologica**\n",
    "- Al contrario di un lemma, la radice di una parola può non essere di senso compiuto\n",
    "- Termini diversi (anche come lemma) possono avere la stessa radice\n",
    "- Ciònonostante, lo stemming è spesso usato come **alternativa alla lemmatizzazione**, in quanto\n",
    "  - termini con la stessa radice sono spesso correlati, pur essendo lemmi diversi (es. nome \"pesce\" e verbo \"pescare\")\n",
    "  - lo stemming non richiede il POS tagging ed è generalmente molto più efficiente\n",
    "- NLTK integra diversi algoritmi di stemming, tra cui ad es. `PorterStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lemmat', 'lemmat', 'lemmat')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "ps.stem(\"lemmatization\"), ps.stem(\"lemmatizer\"), ps.stem(\"lemmatize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creiamo una funzione di segmentazione che applica lo stemming alle parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_stemming(text):\n",
    "    return [ps.stem(token) for token\n",
    "            in nltk.tokenize.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'have', 'shown', 'mani', 'exampl', '!']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_stemming(\"We have shown many examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Addestriamo e testiamo un modello con la solita procedura..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, tokenizer=tokenize_with_stemming)),\n",
    "    (\"classifier\", LogisticRegression(C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15956"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82433333333333336"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment Analysis in NLTK\n",
    "\n",
    "- La stima dell'orientamento positivo o negativo di opinioni scritte è un problema molto comune\n",
    "- Per questo esistono modelli preaddestrati, tra cui [_VADER_](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109) (_Valence Aware Dictionary and sEntiment Reasoner_)\n",
    "- NLTK permette di utilizzare VADER per valutare l'orientamento di opinioni, senza bisogno di addestrare modelli\n",
    "- Per usare VADER, scarichiamo i dati necessari e creiamo un oggetto `SentimentIntensityAnalyzer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pasolini/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasolini/software/miniconda3/envs/dialab/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- VADER si basa su un insieme di regole e su un dizionario (_lexicon_), che associa ad ogni parola un punteggio che ne denota la valenza positiva o negativa\n",
    "- Oltre a parole della lingua inglese, il dizionario prevede anche termini abbreviati, slang e emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"excellent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"sux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"n00b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\":)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per stimare la polarità di una frase, usiamo il metodo `polarity_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.6588, 'neg': 0.0, 'neu': 0.406, 'pos': 0.594}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"What an awesome movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.3804, 'neg': 0.464, 'neu': 0.536, 'pos': 0.0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"It was really boring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I valori `pos`, `neg` e `neu` valutano quanto la frase sia positiva, negativa o neutra\n",
    "- `compound` è un'aggregazione dei tre punteggi e riassume la polarità della frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Valutiamo il modello VADER sul nostro validation set\n",
    "- Creiamo una funzione che associ ad ogni recensione un'etichetta \"pos\" o \"neg\" come i modelli\n",
    "  - usiamo la funzione `sent_tokenize` per scomporre ciascuna recensione nelle singole frasi\n",
    "  - una volta valutate le frasi una per una, si verifica la somma dei loro punteggi `compound`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_review(review):\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    scores = list(map(vader.polarity_scores, sentences))\n",
    "    return \"pos\" if sum(s[\"compound\"] for s in scores) >= 0 else \"neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testiamo la funzione sulle due frasi sopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_review(\"What an awesome movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_review(\"It was really boring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Estraiamo la lista delle classificazioni effettuate da VADER sul validation set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_preds = [label_review(review) for review in reviews_val[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...e valutiamone l'accuratezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73166666666666669"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(reviews_val[\"label\"], vader_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'accuratezza di VADER è di qualche punto inferiore a quella dei nostri modelli\n",
    "- Si consideri che il modello VADER è più orientato all'analisi di testi brevi con largo uso di abbreviazioni ed emoticon (es. Twitter), mentre i modelli addestrati da noi sono specifici per queste recensioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizi\n",
    "\n",
    "1. Confrontare l'accuratezza ottenuta variando il parametro `min_df` di `TfidfVectorizer` da 1 a 5, e il parametro `C` di `LogisticRegression` a 1 e 100\n",
    "  - è ammesso sia usare lo split train/validation già costruito che usare la cross validation con grid search\n",
    "2. Addestrare e testare un modello in cui siano usati come termini solamente gli aggettivi\n",
    "  - usare un tokenizer personalizzato che usi il POS tagging per individuare gli aggettivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression())\n",
    "])\n",
    "gs = GridSearchCV(model, {\"vectorizer__min_df\": [1, 2, 3, 4, 5], \"classifier__C\": [1, 100]})\n",
    "gs.fit(reviews_train.text, reviews_train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_only_adjectives(text):\n",
    "    return [token for token, tag\n",
    "            in nltk.pos_tag(nltk.tokenize.word_tokenize(text))\n",
    "            if tag[0] == \"J\"]\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, tokenizer=tokenize_only_adjectives)),\n",
    "    (\"classifier\", LogisticRegression())\n",
    "])\n",
    "model.fit(reviews_train.text, reviews_train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(reviews_val.text, reviews_val.label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIA (Lab)",
   "language": "python",
   "name": "dialab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
